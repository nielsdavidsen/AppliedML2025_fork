{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of b-quark jets in the Aleph simulated data\n",
    "\n",
    "The following is an introduction to using Machine Learning (ML) - in particular Boosted Decision Trees (BDT) - for trying to determine, if an entry in a data file is of one type (signal, ill, guilty, etc.) or another (background, healthy, innocent, etc.).\n",
    "\n",
    "You may choose between two data samples:\n",
    "1. A particle physics dataset containing simulated decays of the $Z^0$ boson decaying to a quark and an anti-quark producing two \"jets\" of particles. The question is, if the jets are from a b-quark (b-jet) or from lighter quarks (l-jet).\n",
    "3. A \"medical\" dataset which concers a lifestyle disease in relation to various (transformed) lifestyle variables (reduced in number of variables to match the Aleph b-jet data set).\n",
    "\n",
    "In the following, we discuss the problem from the b-jet point of view, as this is where the largest size datasets are available. However, we stress that from the point of view of ML, data content (what is being considered) is not essential to know (for now!!!). And knowing the content in details requires domain knowledge, i.e. that you are an expert in the specific field, that the data comes from. This part is very important, but not the focus in this course.\n",
    "\n",
    "In the end, this exercise is the simple start \"outside ML\" and moving into the territory of Machine Learning analysis.\n",
    "\n",
    "### The Data:\n",
    "The input variables (X) are (used by Aleph for their NN):\n",
    "* **prob_b**: Probability of being a b-jet from the pointing of the tracks to the vertex.\n",
    "* **spheri**: Sphericity of the event, i.e. how spherical it is.\n",
    "* **pt2rel**: The transverse momentum squared of the tracks relative to the jet axis, i.e. width of the jet.\n",
    "* **multip**: Multiplicity of the jet (in a relative measure).\n",
    "* **bqvjet**: b-quark vertex of the jet, i.e. the probability of a detached vertex.\n",
    "* **ptlrel**: Transverse momentum (in GeV) of possible lepton with respect to jet axis (about 0 if no leptons).\n",
    "\n",
    "Auxilary variables (Z) are (not used by Aleph for their NN):\n",
    "* energy: Measured energy of the jet in GeV. Should be 45 GeV, but fluctuates.\n",
    "* cTheta: cos(theta), i.e. the polar angle of the jet with respect to the beam axis. Note, that the detector works best in the central region (|cTheta| small) and less well in the forward regions.\n",
    "* phi:    The azimuth angle of the jet. As the Aleph detector was essentially uniform in phi, this should not matter (much).\n",
    "\n",
    "The target variable (Y) is:\n",
    "* **isb**:    1 if it is from a b-quark and 0, if it is not.\n",
    "\n",
    "Finally, those before you (the Aleph collaboration in the mid 90'ies) produced a Neural Net (6 input variables, two hidden layers with 10 neurons in each, and 1 output varible) based classification variable, which you can compare to (and compete with?):\n",
    "* **nnbjet**: Value of original Aleph b-jet tagging algorithm, using only the last six variables (for reference).\n",
    "\n",
    "In case you choose **the medical data**, the variables to use as input (X) are: **Qsocial, BMI, Roccupat, Rgenetic, Rdietary, and Rhormonn** (reflecting Quantiles and Ratios of medical measurements). The target variable (Y) is (naturally): **TrulyIll**, and you can compare your results to the average of doctors: **DocScore**.\n",
    "\n",
    "\n",
    "## The Task: Input Feature Ranking (here for LightGBM model):\n",
    "\n",
    "The following exercise is about Input Feature Ranking. Your task is to give an overview out of the importance of each variable in the model, and their respective contribution to the models perfomance.\n",
    "\n",
    "\n",
    "\n",
    "* Author: Troels C. Petersen (NBI)\n",
    "* Email:  petersen@nbi.dk\n",
    "* Date:   15th of April 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division   # Ensures Python3 printing & division standard\n",
    "import pandas as pd \n",
    "from pandas import Series, DataFrame \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "SavePlots = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and print the variables:\n",
    "data = pd.DataFrame(np.genfromtxt('../Week1/AlephBtag_MC_train_Nev50000.csv', names=True))\n",
    "# data = pd.DataFrame(np.genfromtxt('../Week1/Medical_Npatients50000.csv', names=True))\n",
    "\n",
    "variables = data.columns\n",
    "print(variables.values)\n",
    "\n",
    "# Decide on which variables to use for input (X) and what defines the label (Y):\n",
    "input_variables = variables[(variables != 'nnbjet') & (variables != 'isb') & (variables != 'energy') & (variables != 'cTheta') & (variables != 'phi')]\n",
    "input_data      = data[input_variables]\n",
    "truth_data      = data['isb']\n",
    "benchmark_data  = data['nnbjet']\n",
    "print(\"  Variables used for training: \", input_variables.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Classification using LightGBM:\n",
    "\n",
    "\n",
    "This is a solution example using LightGBM (tree based)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "import time\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "# Split data set into training and test set. We choose a 75:25 division here. \n",
    "\n",
    "# Dataset is shuffeled before the split (to avoid any ordering). By using a fixed\n",
    "# random seed number (42), we can rerun and obtain the same result (for reproducibility!).\n",
    "X_train, x_test, Y_train, y_test, benchmark_train, benchmark_test = \\\n",
    "    train_test_split(input_data, truth_data, benchmark_data, test_size=0.25, random_state=42)\n",
    "\n",
    "# Feed the datasets to LightGBM:\n",
    "lgb_train = lgb.Dataset(X_train, Y_train)\n",
    "lgb_eval  = lgb.Dataset(x_test,  y_test, reference=lgb_train)\n",
    "\n",
    "# Set parameters for LightGBM (known more generally as \"hyper parameters\"):\n",
    "params = {\n",
    "    'boosting_type': 'gbdt', # Traditional Gradient Boosting tree, we are combining many 'weak' learners here!\n",
    "    'objective': 'binary',   # The outcome is binary, b-quark or not\n",
    "    'num_leaves': 6,         # Set a maximum tree leaves to avoid overfitting\n",
    "    'verbose': 1,            # Level of output. Can be set to -1 to suppress the output\n",
    "}\n",
    "\n",
    "# Train the model:\n",
    "gbm = lgb.train(params,                             # General settings (defined above)\n",
    "                lgb_train,                          # Data to use for training\n",
    "                num_boost_round=1000,               # How many rounds for training\n",
    "                valid_sets=lgb_eval,                # Data to use for validation\n",
    "                callbacks=[early_stopping(20)])     # Stops if no improvement is seen in N=20 rounds.\n",
    "\n",
    "# Make predictions.\n",
    "# NOTE the difference between 'score' (continuous in ]0,1[) and 'predictions' (integer: 0 or 1):\n",
    "# Also NOTE that you can choose where to set the threshold (here set to 0.1)\n",
    "y_score = gbm.predict(x_test, num_iteration=gbm.best_iteration)  # Scores are floats in the range ]0,1[.\n",
    "y_pred  = [1 if pred > 0.1 else 0 for pred in y_score]               # Classify b-quark yes or no (for comparison). \n",
    "\n",
    "# Print the time usage:\n",
    "end = time.time()\n",
    "print(f\"Time used by LightGBM: {(end-start)*1000:.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate:\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)                  # False/True Positive Rate for our model\n",
    "fpr_nnbjet, tpr_nnbjet, _ = roc_curve(y_test, benchmark_test)  # False/True Positive Rate for Aleph NNbjet\n",
    "\n",
    "# We can now calculate the Area-Under-the-Curve (AUC) scores of these ROC-curves:\n",
    "auc_score = auc(fpr,tpr)                        # This is the AUC score for our model\n",
    "auc_score_nnbjet = auc(fpr_nnbjet, tpr_nnbjet)  # This is the AUC score for Aleph NNbjet\n",
    "\n",
    "# Let's plot the ROC curves for these results:\n",
    "fig = plt.figure(figsize = [10,10])\n",
    "plt.title('Model Comparison (ROC curves)', size = 16)\n",
    "plt.plot(fpr, tpr, label=f'Our LightGBM model (AUC = {auc_score:5.3f})')\n",
    "plt.plot(fpr_nnbjet, tpr_nnbjet, label = f'Aleph NNbjet (AUC = {auc_score_nnbjet:5.3f})')\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel('False Postive Rate', size=16)\n",
    "plt.ylabel('True Positive Rate', size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "1. Try to determine an input feature ranking first by using the **build-in function** for this in LightGBM. Yes, look at the documentation and find the one line of code that does this. Can you also find - and understand - the description of how it reaches these results?\n",
    "\n",
    "2. Now try to get an input feature ranking using **permutation invariance**. Think of an ML package that contains a lot of different ML tools, and check if this package does not have a tool for doing so. Do you get a similar ranking as for the above method?\n",
    "\n",
    "3. Finally, calculate SHAP values and use these (using Sum_i |SHAP_i |) to determine a ranking of the input features. Again, compare it to the two above. Do they agree?\n",
    "\n",
    "4. Try to rerun the model twice with only 5 input parameters: Missing the highest and lowest ranking feature. Do you see a significant difference in the performance between these two cases? And how about when comparing to the 6 input feature case?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning points:\n",
    "\n",
    "1. You should first of all understand the concept of “feature ranking”.\n",
    "\n",
    "2. You should also be capable of determining the feature ranking for a given algorithm in multiple ways, specifically using permutation invariance (PI) and SHAP values.\n",
    "\n",
    "3. Finally, you should understand that while PI and other build-in methods gives an average input feature ranking, the SHAP values are capable of giving an individual input feature ranking for each case.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
