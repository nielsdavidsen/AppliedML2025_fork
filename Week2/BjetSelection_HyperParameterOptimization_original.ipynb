{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of b-quark jets in the Aleph simulated data\n",
    "\n",
    "The following is an introduction to using Machine Learning (ML) - in particular Boosted Decision Trees (BDT) - for trying to determine, if an entry in a data file is of one type (signal, ill, guilty, etc.) or another (background, healthy, innocent, etc.).\n",
    "\n",
    "You may choose between two data samples:\n",
    "1. A particle physics dataset containing simulated decays of the $Z^0$ boson decaying to a quark and an anti-quark producing two \"jets\" of particles. The question is, if the jets are from a b-quark (b-jet) or from lighter quarks (l-jet).\n",
    "3. A \"medical\" dataset which concers a lifestyle disease in relation to various (transformed) lifestyle variables (reduced in number of variables to match the Aleph b-jet data set).\n",
    "\n",
    "In the following, we discuss the problem from the b-jet point of view, as this is where the largest size datasets are available. However, we stress that from the point of view of ML, data content (what is being considered) is not essential to know (for now!!!). And knowing the content in details requires domain knowledge, i.e. that you are an expert in the specific field, that the data comes from. This part is very important, but not the focus in this course.\n",
    "\n",
    "In the end, this exercise is the simple start \"outside ML\" and moving into the territory of Machine Learning analysis.\n",
    "\n",
    "### The Data:\n",
    "The input variables (X) are (used by Aleph for their NN):\n",
    "* **prob_b**: Probability of being a b-jet from the pointing of the tracks to the vertex.\n",
    "* **spheri**: Sphericity of the event, i.e. how spherical it is.\n",
    "* **pt2rel**: The transverse momentum squared of the tracks relative to the jet axis, i.e. width of the jet.\n",
    "* **multip**: Multiplicity of the jet (in a relative measure).\n",
    "* **bqvjet**: b-quark vertex of the jet, i.e. the probability of a detached vertex.\n",
    "* **ptlrel**: Transverse momentum (in GeV) of possible lepton with respect to jet axis (about 0 if no leptons).\n",
    "\n",
    "Auxilary variables (Z) are (not used by Aleph for their NN):\n",
    "* energy: Measured energy of the jet in GeV. Should be 45 GeV, but fluctuates.\n",
    "* cTheta: cos(theta), i.e. the polar angle of the jet with respect to the beam axis. Note, that the detector works best in the central region (|cTheta| small) and less well in the forward regions.\n",
    "* phi:    The azimuth angle of the jet. As the Aleph detector was essentially uniform in phi, this should not matter (much).\n",
    "\n",
    "The target variable (Y) is:\n",
    "* **isb**:    1 if it is from a b-quark and 0, if it is not.\n",
    "\n",
    "Finally, those before you (the Aleph collaboration in the mid 90'ies) produced a Neural Net (6 input variables, two hidden layers with 10 neurons in each, and 1 output varible) based classification variable, which you can compare to (and compete with?):\n",
    "* **nnbjet**: Value of original Aleph b-jet tagging algorithm, using only the last six variables (for reference).\n",
    "\n",
    "In case you choose **the medical data**, the variables to use as input (X) are: **Qsocial, BMI, Roccupat, Rgenetic, Rdietary, and Rhormonn** (reflecting Quantiles and Ratios of medical measurements). The target variable (Y) is (naturally): **TrulyIll**, and you can compare your results to the average of doctors: **DocScore**.\n",
    "\n",
    "\n",
    "## The Task this weak:\n",
    "\n",
    "### HyperParameter optimization (here for LightGBM model):\n",
    "\n",
    "The following exercise is about HyperParameter (HP) optimization. Your task is to find the HPs that gives the best performance for your model. In order to avoid statistical fluctuations we use 50000 events (but you may eventually decide to use more).\n",
    "\n",
    "The are many things to tune in LigthGBM, see this long [list of HPs for LightGBM algorithm](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html). In order to limit ourselves, you should in the following only consider (at least to begin with) the following five HPs:\n",
    "\n",
    "* Number of tree\n",
    "* Number of leaves\n",
    "* Learning rate\n",
    "* Boosting (how to combine trees: 1. Traditional Gradient Boosting, Random Forest, or [DART](https://arxiv.org/abs/1505.01866))\n",
    "* Minimum data in each leave (use this to avoid over-fitting)\n",
    "\n",
    "The purpose is to get the best possible performance out.\n",
    "\n",
    "\n",
    "* Author: Troels C. Petersen (NBI)\n",
    "* Email:  petersen@nbi.dk\n",
    "* Date:   15th of April 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division   # Ensures Python3 printing & division standard\n",
    "import pandas as pd \n",
    "from pandas import Series, DataFrame \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "SavePlots = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and print the variables:\n",
    "data = pd.DataFrame(np.genfromtxt('../Week1/AlephBtag_MC_train_Nev50000.csv', names=True))\n",
    "# data = pd.DataFrame(np.genfromtxt('../Week1/Medical_Npatients50000.csv', names=True))\n",
    "\n",
    "variables = data.columns\n",
    "print(variables.values)\n",
    "\n",
    "# Decide on which variables to use for input (X) and what defines the label (Y):\n",
    "input_variables = variables[(variables != 'nnbjet') & (variables != 'isb') & (variables != 'energy') & (variables != 'cTheta') & (variables != 'phi')]\n",
    "input_data      = data[input_variables]\n",
    "truth_data      = data['isb']\n",
    "benchmark_data  = data['nnbjet']\n",
    "print(\"  Variables used for training: \", input_variables.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set-up\n",
    "First we import the necessary packages and split the data set into training and test set. We choose a 75:25 division here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "import time\n",
    "\n",
    "# Dataset is shuffeled before the split (to avoid any ordering). By using a fixed\n",
    "# We choose a random seed number (42), so that  we can rerun and obtain the same result (for reproducibility!).\n",
    "input_train, input_test, truth_train, truth_test, benchmark_train, benchmark_test = \\\n",
    "    train_test_split(input_data, truth_data, benchmark_data, test_size=0.25, random_state=42)\n",
    "\n",
    "# Feed the datasets to LightGBM:\n",
    "lgb_train = lgb.Dataset(input_train, truth_train)\n",
    "lgb_eval  = lgb.Dataset(input_test,  truth_test, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of hyperparameters \n",
    "Now, comes the important part: **Choosing the right hyperparameters**\n",
    "\n",
    "1. <code>boosting</code>: (default = gbdt) <br>\n",
    "    <code>gbdt</code> --> traditional Gradient Boosting Decision Tree<br>\n",
    "    <code>rf</code> --> Random Forest (bagging, not boosting)<br>\n",
    "    <code>dart</code> --> [Dropouts meet Multiple Additive Regression Trees](https://arxiv.org/abs/1505.01866) (drops some trees during training) <br>\n",
    "\n",
    "2. <code>learning_rate</code>: (default = 0.1)<br>\n",
    "    Determines how much each new tree contributes to the final prediction<br>\n",
    "    Must be bigger than 0 <br>\n",
    "\n",
    "3. <code>num_leaves</code>: (default=31)<br>\n",
    "    Controls the complexity of individual trees. More leaves = more complex trees<br>\n",
    "    More leaves improve the training accuracy but increase the risk of overfitting<br>\n",
    "\n",
    "4. <code>num_treas</code>: (default=100)<br>\n",
    "    The total number of trees in the model<br>\n",
    "    More leaves improve the training accuracy but increase the risk of overfitting<br>\n",
    "\n",
    "\n",
    "5. <code>min_data_in_leaf</code>: (default=20)<br>\n",
    "    Minimum number of samples a leaf must have.<br>\n",
    "    This prevents the trees from becoming too specific to training data<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for LightGBM (known more generally as \"hyper parameters\"):\n",
    "params = {\n",
    "    'boosting_type': 'gbdt', \n",
    "    'learning_rate': 0.1,     \n",
    "    'num_leaves': 31,         \n",
    "    'num_trees': 100,\n",
    "    'min_data_in_leaf': 20,  \n",
    "    'objective': 'binary',   # The outcome is binary, b-quark or not\n",
    "    'verbose': 1,            # Level of output. Can be set to -1 to suppress the output\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Let's train the Decision tree using the parameters from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We track the time it takes to train the model:\n",
    "start=time.time()\n",
    "\n",
    "# Train the model:\n",
    "gbm = lgb.train(params,                             # General settings (defined above)\n",
    "                lgb_train,                          # Data to use for training\n",
    "                num_boost_round=1000,               # How many rounds for training\n",
    "                valid_sets=lgb_eval,                # Data to use for validation\n",
    "                callbacks=[early_stopping(20)])     # Stops if no improvement is seen in N=20 rounds.\n",
    " \n",
    "# Print the time usage:\n",
    "end = time.time()\n",
    "print(f\"\\nTime used by LightGBM: {(end-start):.1f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Let's make some predictions and plot a ROC Cuve to compare the performance of our model to Aleph's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "# NOTE the difference between 'score' (continuous in ]0,1[) and 'predictions' (integer: 0 or 1):\n",
    "# Also NOTE that you can choose where to set the threshold (here set to 0.1)\n",
    "y_score = gbm.predict(input_test, num_iteration=gbm.best_iteration)  # Scores are floats in the range ]0,1[.\n",
    "y_pred  = [1 if pred > 0.1 else 0 for pred in y_score]               # Classify b-quark yes or no (for comparison).\n",
    "\n",
    "# Evaluate:\n",
    "fpr, tpr, _ = roc_curve(truth_test, y_score)                  # False/True Positive Rate for our model\n",
    "fpr_nnbjet, tpr_nnbjet, _ = roc_curve(truth_test, benchmark_test)  # False/True Positive Rate for Aleph NNbjet\n",
    "\n",
    "# We can now calculate the Area-Under-the-Curve (AUC) scores of these ROC-curves:\n",
    "auc_score = auc(fpr,tpr)                        # This is the AUC score for our model\n",
    "auc_score_nnbjet = auc(fpr_nnbjet, tpr_nnbjet)  # This is the AUC score for Aleph NNbjet\n",
    "\n",
    "# Let's plot the ROC curves for these results:\n",
    "fig = plt.figure(figsize = [10,10])\n",
    "plt.title('Model Comparison (ROC curves)', size = 16)\n",
    "plt.plot(fpr, tpr, label=f'Our LightGBM model (AUC = {auc_score:5.3f})')\n",
    "plt.plot(fpr_nnbjet, tpr_nnbjet, label = f'Aleph NNbjet (AUC = {auc_score_nnbjet:5.3f})')\n",
    "plt.legend(fontsize=16)\n",
    "plt.xlabel('False Postive Rate', size=16)\n",
    "plt.ylabel('True Positive Rate', size=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "How well can you optimize the training/performance of your algorithm?\n",
    "\n",
    "1. First try to simply do, what you might already have done: Change the HyperParameters (HPs) by hand (sometimes refered to as \"Graduate student optimization\"), and see if helps.\n",
    "\n",
    "2. **Grid search**: Systematically try every combination of values from a predefined grid. If you test `n` values for each of `k` parameters, how many combinations will you end up having to test? Is this prohibitively large?<br>\n",
    "NOTE: Check if Grid Search for HPs is a build in function before you code it up yourself.\n",
    "\n",
    "3. **Random search**: Try 10 random combinations of parameters and keep the best.\n",
    "NOTE: Check if Random Search for HPs is a build in function before you code it up yourself.\n",
    "\n",
    "\n",
    "4. Which of the above approaches did you find most effective?\n",
    "\n",
    "5. Are the best-performing solutions for the three search strategies clustered in a certain region of the parameter space? Try plotting the performance for a few values for each parameter dimension.\n",
    "\n",
    "6. What happens if you remove Early Stopping?\n",
    "\n",
    "7. How can you use Bayesian optimization to improve performance with less HPs testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning points:\n",
    "\n",
    "From this exercise you should:\n",
    "\n",
    "- Know what hyperparameters are, and that they are important for optimizing ML algorithms.\n",
    "- Know that grid search becomes computationally expensive quickly.\n",
    "- Understand that random searchers can be surprisingly effective â€” and scales much better than grid search in high dimensions.\n",
    "- Understand that the hard thing about random searches is figuring out the _right_ HP ranges \n",
    "- Understand that some parameters may be worth exploring more than others because they have a greater impact\n",
    "- Realise that early stopping is great as it avoids overfitting and reduces run time\n",
    "- Have at least a conceptual understanding that Bayesian optimization learns from past trials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
