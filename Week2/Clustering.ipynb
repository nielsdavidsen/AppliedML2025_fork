{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K means clustering\n",
    "\n",
    "Clustering is used for classification. Here we are going to work with simulated data. We are going to simulate a set of people; children, women and men. We will assume that children are small, in height and weight, women slightly larger and men larger again. We will simulate data with 20% children, 45% women and 35% men. We will assume that weight is correlated to height.\n",
    "\n",
    "Note how the following code does not use any imported algorithms. You may of course do so, and also change the number of clusters.\n",
    "\n",
    "---\n",
    "\n",
    "* Author: Troels C. Petersen (NBI) & Brian Vinter (former NBI, now AU)\n",
    "* Email:  petersen@nbi.dk\n",
    "* Date:   25th of April 2025 (latest version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce/simulate the data:\n",
    "import numpy\n",
    "\n",
    "children, women, men = 20, 45, 35\n",
    "sample = children + women + men\n",
    "\n",
    "height_children = numpy.random.normal(120, 15, children) / 100\n",
    "weight_children = 21.5 * height_children * numpy.random.normal(1.0, 0.05, children)\n",
    "\n",
    "height_women = numpy.random.normal(170, 5, women) / 100\n",
    "weight_women = 40.0 * height_women * numpy.random.normal(1.0, 0.1, women)\n",
    "\n",
    "height_men = numpy.random.normal(180, 5, men) / 100\n",
    "weight_men = 50.0 * height_men * numpy.random.normal(1.0, 0.1, men)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the data so that we may compare to our later results in classifying our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(height_children, weight_children, 'go')\n",
    "plt.plot(height_women, weight_women, 'ro')\n",
    "plt.plot(height_men, weight_men, 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we place all data into one large array. Thereby we loose all knowledge og categories, though we still know from the order of the points, we can use that fact later for testing. We move on to normalize the data for use in the distance function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = numpy.concatenate((numpy.array((height_children, weight_children)), \\\n",
    "                              numpy.array((height_women, weight_women)) , \\\n",
    "                              numpy.array((height_men, weight_men))), axis = 1)\n",
    "\n",
    "# We normalize and transpose to have the data in rows\n",
    "data = raw_data / numpy.max(raw_data, axis = 1)[numpy.newaxis].T \n",
    "plt.plot(data[0, :], data[1, :], 'yo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start trying to find the clusters. We start out by picking 3 random points and build clusters around those. This will often look good, even though we are only building random clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_dist(observation, data):\n",
    "    return numpy.sqrt((data[0, :] - observation[0])**2 + (data[1, :] - observation[1])**2)\n",
    "\n",
    "k=3\n",
    "centroids = numpy.array([data[:, numpy.random.randint(sample)] for _ in range(k)])\n",
    "distances = numpy.empty((k,sample))\n",
    "for d in range(k):\n",
    "    distances[d, :] = all_dist(centroids[d], data)\n",
    "winners = numpy.argmin(distances, axis = 0)\n",
    "clusters = [data[:, winners == i] for i in range(k)]\n",
    "for cluster, color in zip(clusters, ['go', 'ro', 'bo']):\n",
    "    plt.plot(cluster[0, :], cluster[1, :], color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously we need to iterate to make the clusters better. We do this by recalculating the center of a cluster to be the mean of the points insie the cluster for each itteration. Once the cluster centers do not change we have converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(data, k):\n",
    "    centroids = numpy.array([data[:, numpy.random.randint(sample)] for _ in range(k)])\n",
    "    done = False\n",
    "    while not done:\n",
    "        distances = numpy.empty((k,sample))\n",
    "        for d in range(k):\n",
    "            distances[d, :] = all_dist(centroids[d], data)\n",
    "        winners = numpy.argmin(distances, axis = 0)\n",
    "        clusters = [data[:, winners == i] for i in range(k)]\n",
    "        prev_centroids = centroids\n",
    "        centroids = numpy.array([numpy.average(cluster, axis = 1) for cluster in clusters])\n",
    "        if numpy.sum(prev_centroids-centroids) == 0:\n",
    "            done=True\n",
    "    for cluster, color in zip(clusters, ['go', 'ro', 'bo']):\n",
    "        plt.plot(cluster[0, :], cluster[1, :], color)\n",
    "    plt.show()\n",
    "\n",
    "cluster(data,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excersises:\n",
    "1. Increase the sample size and see how that affects the algorithm.\n",
    "2. Try and add on obvious outlier; a 1m 100kg person for instance, and see what happens.\n",
    "3. Try other (imported) algorithms on this data sample, possibly also changing its parameters and size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning points:\n",
    "0. Clustering needs normalisation of variables to begin with.\n",
    "1. Clustering is not easy and does not always yield a great result.\n",
    "2. Different methods work on different datasets - it is not clear from the data which algorithm to use.\n",
    "3. Visualisation (whenever possible) is important.\n",
    "4. Evaluating the performance and (sub)selecting variables is not easy.\n",
    "5. The \"elbow method\" can be used for determining the number of clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
