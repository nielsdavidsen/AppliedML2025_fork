{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of b-quark jets in the Aleph simulated data\n",
    "\n",
    "The following is an introduction to using Machine Learning (ML) - in particular Boosted Decision Trees (BDT) - for trying to determine, if an entry in a data file is of one type (signal, ill, guilty, etc.) or another (background, healthy, innocent, etc.).\n",
    "\n",
    "You may choose between two data samples:\n",
    "1. A particle physics dataset containing simulated decays of the $Z^0$ boson decaying to a quark and an anti-quark producing two \"jets\" of particles. The question is, if the jets are from a b-quark (b-jet) or from lighter quarks (l-jet).\n",
    "3. A \"medical\" dataset which concers a lifestyle disease in relation to various (transformed) lifestyle variables (reduced in number of variables to match the Aleph b-jet data set).\n",
    "\n",
    "In the following, we discuss the problem from the b-jet point of view, as this is where the largest size datasets are available. However, we stress that from the point of view of ML, data content (what is being considered) is not essential to know (for now!!!). And knowing the content in details requires domain knowledge, i.e. that you are an expert in the specific field, that the data comes from. This part is very important, but not the focus in this course.\n",
    "\n",
    "In the end, this exercise is the simple start \"outside ML\" and moving into the territory of Machine Learning analysis.\n",
    "\n",
    "### The Data:\n",
    "The input variables (X) are (used by Aleph for their NN):\n",
    "* **prob_b**: Probability of being a b-jet from the pointing of the tracks to the vertex.\n",
    "* **spheri**: Sphericity of the event, i.e. how spherical it is.\n",
    "* **pt2rel**: The transverse momentum squared of the tracks relative to the jet axis, i.e. width of the jet.\n",
    "* **multip**: Multiplicity of the jet (in a relative measure).\n",
    "* **bqvjet**: b-quark vertex of the jet, i.e. the probability of a detached vertex.\n",
    "* **ptlrel**: Transverse momentum (in GeV) of possible lepton with respect to jet axis (about 0 if no leptons).\n",
    "\n",
    "Auxilary variables (Z) are (not used by Aleph for their NN):\n",
    "* energy: Measured energy of the jet in GeV. Should be 45 GeV, but fluctuates.\n",
    "* cTheta: cos(theta), i.e. the polar angle of the jet with respect to the beam axis. Note, that the detector works best in the central region (|cTheta| small) and less well in the forward regions.\n",
    "* phi:    The azimuth angle of the jet. As the Aleph detector was essentially uniform in phi, this should not matter (much).\n",
    "\n",
    "The target variable (Y) is:\n",
    "* **isb**:    1 if it is from a b-quark and 0, if it is not.\n",
    "\n",
    "Finally, those before you (the Aleph collaboration in the mid 90'ies) produced a Neural Net (6 input variables, two hidden layers with 10 neurons in each, and 1 output varible) based classification variable, which you can compare to (and compete with?):\n",
    "* **nnbjet**: Value of original Aleph b-jet tagging algorithm, using only the last six variables (for reference).\n",
    "\n",
    "In case you choose **the medical data**, the variables to use as input (X) are: **Qsocial, BMI, Roccupat, Rgenetic, Rdietary, and Rhormonn** (reflecting Quantiles and Ratios of medical measurements). The target variable (Y) is (naturally): **TrulyIll**, and you can compare your results to the average of doctors: **DocScore**.\n",
    "\n",
    "\n",
    "### The Task:\n",
    "Thus, the task before you is to produce functions (non-ML and ML algorithm), which given the input variables X provides an output variable estimate, Y_est, which is \"closest possible\" to the target variable, Y. The \"closest possible\" is left to the user to define in a _Loss Function_, which we will discuss further later. In classification problems (such as this), the typical loss function to use is [\"Cross Entropy\"](https://en.wikipedia.org/wiki/Cross_entropy).\n",
    "\n",
    "You should approach the problem in two ways:\n",
    "* Initially (i.e. first 15-30 minutes), simply with \"if\"-statements making requirements on certain variables. This corresponds to selecting \"boxes\" in the input variable space (typically called \"X\"). One could also try a linear combination of input variables (i.e. a Fisher discriminant), which corresponds to a plane in the X-space. Such a solution is fast and transparent (very good), but as the data contains non-linear correlations, it is likely to approximate.<br>\n",
    "**The point of this step is NOT to make a great algorith, but rather to set up comparing different algorithms!**\n",
    "\n",
    "* Next using Machine Learning (ML) methods. We will during the first week try both Boosted Decision Tree (BDT) based and Neural Net (NN) based methods, and see how complicated (or not) it is to get a good solution, and how much better it performs compared to the \"classic\" selection method.\n",
    "\n",
    "Once you obtain a classification of b-jets vs. non-b-jets, think about how to quantify the quality of your algorithm. Once you have a \"metric\" for doing so, try to compare it to the NN result of the Aleph collaboration, given by the variable \"nnbjet\". It is based on a neural network with 6 input variables (prob_b, spheri, pt2rel, multip, bqvjet, and ptlrel), and two hidden layers each with 10 nodes in. Can you do better?\n",
    "\n",
    "Don't hold back in drawing inspiration from \"ML_MethodsDemos.ipynb\" (in Week0) or the vast internet. BDT suggestions might be [XGBoost](https://xgboost.readthedocs.io/en/release_3.0.0/) or [LightGBM](https://lightgbm.readthedocs.io/en/stable/). NN suggestions might be [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.tensorflow.org/).\n",
    "\n",
    "* Author: Troels C. Petersen (NBI)\n",
    "* Email:  petersen@nbi.dk\n",
    "* Date:   15th of April 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division   # Ensures Python3 printing & division standard\n",
    "import pandas as pd \n",
    "from pandas import Series, DataFrame \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "SavePlots = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and inspect the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and print the variables:\n",
    "data = pd.DataFrame(np.genfromtxt('AlephBtag_MC_train_Nev5000.csv', names=True))\n",
    "# data = pd.DataFrame(np.genfromtxt('Medical_Npatients5000.csv', names=True))\n",
    "\n",
    "variables = data.columns\n",
    "print(variables.values)\n",
    "\n",
    "# Decide on which variables to use for input (X) and what defines the label (Y):\n",
    "input_variables = variables[(variables != 'nnbjet') & (variables != 'isb') & (variables != 'energy') & (variables != 'cTheta') & (variables != 'phi')]\n",
    "input_data      = data[input_variables]\n",
    "truth_data      = data['isb']\n",
    "benchmark_data  = data['nnbjet']\n",
    "print(\"  Variables used for training: \", input_variables.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the distribution of the input variables for each event type:\n",
    "mask = (data['isb'] == 1)      # Mask to divide the event types\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "hist_prob_b_bjets = ax.hist(data['prob_b'][mask],  bins=100, range=(0.0, 1.0), histtype='step', linewidth=2, label='prob_b_bjets', color='blue')\n",
    "hist_prob_b_ljets = ax.hist(data['prob_b'][~mask], bins=100, range=(0.0, 1.0), histtype='step', linewidth=2, label='prob_b_ljets', color='red')\n",
    "ax.set_xlabel(\"Probability of b-quark based on track impact parameters\")\n",
    "ax.set_ylabel(\"Frequency / 0.01\")\n",
    "ax.set_title(\"Distribution of prob_b\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend(loc='best')\n",
    "ax.grid(axis='y')\n",
    "fig.tight_layout()\n",
    "\n",
    "if SavePlots :\n",
    "    fig.savefig('Hist_prob_b.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection:\n",
    "\n",
    "Imagine that you had never heard of Machine Learning (ML), and had to select b-jets without! This would probably boil down to doing so simply based on \"if\"-sentences, as shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the variables logic names, and define them early, so that they only need to be changed\n",
    "# in ONE place (also to ensures consistency!):\n",
    "cut_prop_b  = 0.15\n",
    "\n",
    "# If prob_b indicate b-quark, call it a b-quark, otherwise not! You can expand on this yourself at will.\n",
    "bquark=[]\n",
    "for i in np.arange(len(data['prob_b'])):\n",
    "    if (data['prob_b'][i] > cut_prop_b) : bquark.append(1)\n",
    "    else : bquark.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(bquark) :\n",
    "    N = [[0,0], [0,0]]   # Make a list of lists (i.e. confusion matrix) for counting successes/failures.\n",
    "    for i in np.arange(len(data['isb'])):\n",
    "        if (bquark[i] == 0 and data['isb'][i] == 0) : N[0][0] += 1\n",
    "        if (bquark[i] == 0 and data['isb'][i] == 1) : N[0][1] += 1\n",
    "        if (bquark[i] == 1 and data['isb'][i] == 0) : N[1][0] += 1\n",
    "        if (bquark[i] == 1 and data['isb'][i] == 1) : N[1][1] += 1\n",
    "    fracWrong = float(N[0][1]+N[1][0])/float(len(data['isb']))\n",
    "    accuracy = 1.0 - fracWrong\n",
    "    return N, accuracy, fracWrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, accuracy, fracWrong = evaluate(bquark)\n",
    "print(\"\\nRESULT OF HUMAN ATTEMPT AT A SELECTION:\")\n",
    "print(\"  First number in parenthesis is the estimate, second is the MC truth (i.e. label):\")\n",
    "print(\"  True-Negative (0,0)  = \", N[0][0])\n",
    "print(\"  False-Negative (0,1) = \", N[0][1])\n",
    "print(\"  False-Positive (1,0) = \", N[1][0])\n",
    "print(\"  True-Positive (1,1)  = \", N[1][1])\n",
    "print(\"    Fraction wrong            = ( (0,1) + (1,0) ) / sum = \", fracWrong)\n",
    "print(\"    Fraction right (accuracy) = ( (0,0) + (1,1) ) / sum = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with Aleph NN-approach from 1990'ies (choosing a reasonable selection cut on \"nnbjet\"):\n",
    "bquark=[]\n",
    "for i in np.arange(len(data['nnbjet'])):\n",
    "    if   (data['nnbjet'][i] > 0.82) : bquark.append(1)\n",
    "    else : bquark.append(0)\n",
    "\n",
    "N, accuracy, fracWrong = evaluate(bquark)\n",
    "print(\"\\nALEPH BJET TAG:\")\n",
    "print(\"  First number in parenthesis is the estimate, second is the MC truth (i.e. label):\")\n",
    "print(\"  True-Negative (0,0)  = \", N[0][0])\n",
    "print(\"  False-Negative (0,1) = \", N[0][1])\n",
    "print(\"  False-Positive (1,0) = \", N[1][0])\n",
    "print(\"  True-Positive (1,1)  = \", N[1][1])\n",
    "print(\"    Fraction wrong            = ( (0,1) + (1,0) ) / sum = \", fracWrong)\n",
    "print(\"    Fraction right (accuracy) = ( (0,0) + (1,1) ) / sum = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "1. Start by plotting the six \"Aleph classification variables\" and calculate their correlation matrices for signal and background, and see which seems to separate and correlate most (optional).\n",
    "\n",
    "2. Above, the scoring is simply done by considering the fraction of wrong estimates. Think about what the alternatives could be, especially if you were to give a continuous score like the Aleph-NN. This scoring is called the \"loss function\".\n",
    "\n",
    "3. <b>Draw ROC curves for each of these six separately, to quantify the separation. Also draw the Aleph-NN variable (nnbjet).</b> Is the Aleph-NN better? Are any of the variables \"seemingly worthless\"? (we will return to this point later).<br>\n",
    "Note that decision variables (i.e. 0 and 1) yield a single point in the ROC curve plot.\n",
    "\n",
    "5. <b>Try to get a BDT to make predictions based on the six given input variables. Plot the result in the ROC curve, and see if you managed to compete or even improve upon the Aleph-NN scores.</b>\n",
    "\n",
    "6. <b>Think about what data you based your predictions on? Was it the same data that you trained the algorithm on? And if that is the case, is that reasonable?<b>\n",
    "\n",
    "7. Does including more data (50000 instead of 5000 events) improve your performance?\n",
    "\n",
    "8. Does including the kinematic variables 'energy', 'cTheta', and 'phi' (which are not related to jet type) improve your performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning points:\n",
    "\n",
    "From this exercise you should:\n",
    "\n",
    "1. get a feel for the problem at hand, namely to separate two populations in a 6-dimensional space. It is hard to imagine, yet with simple cuts you should be able to get \"some performance\", though never close to the Aleph-NN.\n",
    "\n",
    "2. learn, that it is hard to do \"by hand\", but that at least the fact that you have known cases (i.e. labels) makes you capable of getting some performance.\n",
    "\n",
    "3. be able to draw ROC curves to compare performances between different variables and scores (floating or discrete).\n",
    "\n",
    "4. know that you can (typically) improve the performance through the use of Machine Learning (ML).\n",
    "\n",
    "5. <b>be capable not only of getting ML results, but also confident in optimising them, and certainly proficient in interpreting them.</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
