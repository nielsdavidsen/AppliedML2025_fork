{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5400e5f",
   "metadata": {},
   "source": [
    "# Autoencoder - on the MNIST dataset:\n",
    "\n",
    "The following is an illustration/exercise in using an Auto-Encoder on image data. The general idea is, that if one can encode (i.e. represent with \"few\" parameters) images, and then from this encoding decode them again to obtain \"almost\" the same images, then the few parameters (in a latent space) contains the basic information about the images.\n",
    "\n",
    "![](AutoEncoder.png \"\")\n",
    "\n",
    "An AutoEncoder is trained by requiring that the output images match the input images *best possible*, which is (of course) represented by a loss function. Thus, **an AutoEncoder does not require labels**, and is as such unsupervised learning. The power lies in that one gets a good representation of the data through the latent space (the size of which you can choose) for essentially any NN-based method, here a CNN.\n",
    "\n",
    "This can then be used (aggresively!) for many things such as:\n",
    "* Compression (with loss!) of the images (to the size of the latent space)\n",
    "* De-noising images\n",
    "* Anomaly detection\n",
    "* Unsupervised learning (e.g. clustering) on images, sound, graphs, etc.\n",
    "\n",
    "In the example at hand, we consider the MNIST dataset (28x28 (=784) images of digits), and autoencode these images into a latent space of size 4. We then project this latent space onto just two dimensions using UMAP, and see if we (or a clustering algorithm) can detect clusters and possibly determine how many digits there are... *without ever having known anything about arabic numerals!!!*\n",
    "\n",
    "***\n",
    "\n",
    "Authors: Janni Nikolaides and Troels Petersen<br>\n",
    "Date: 11th of May 2025 (latest version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f30681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_dataset = datasets.MNIST('dataset', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST('dataset', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into training and validation data:\n",
    "full_len = len(train_dataset)\n",
    "train_data, val_data = random_split(train_dataset, [int(full_len-full_len*0.2), int(full_len*0.2)])\n",
    "batch_size=256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc205c",
   "metadata": {},
   "source": [
    "### Question - do we need to split the data?\n",
    "\n",
    "We somehow always split the data into these three datasets, but given that we are doing unsupervised learning here, try to think about the degree to which we need to divide the data in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049398c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch dataloaders for data:\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "valid_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aedb61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder model (explained in comments below):\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        ### Convolutional section:\n",
    "        # So we decide to take our 1 input image and first apply 8 kernels/filters of size 3x3\n",
    "        # with a stride (i.e. step size) of 2 and padding the edge with one layer.\n",
    "        # This output goes through a ReLU and then through 16 new kernels/filters\n",
    "        # of the same size, stride and padding, which is then BatchNorm'ed, ReLU'ed, and finally\n",
    "        # convoluted a third time, again doubling the number of kernels/filters (quite standard!).\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        ### Flatten layer\n",
    "        # The images/matrices are then flatten'ed (put into one long array), put through a linear layer,\n",
    "        # then a ReLU, and then finally another linear layer, which boils it down to the latent space\n",
    "        # dimension.\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        ### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            nn.Linear(3 * 3 * 32, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, encoded_space_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder_cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.encoder_lin(x)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
    "        super().__init__()\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 3 * 3 * 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        ### The decoder does the exact opposite, reconstructing the images from the latent space values.\n",
    "        self.unflatten = nn.Unflatten(dim=1, \n",
    "        unflattened_size=(32, 3, 3))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and training parameters:\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer (both for the encoder and the decoder!)\n",
    "lr = 0.001\n",
    "\n",
    "# Set the random seed for reproducible results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Initialize the two networks\n",
    "d = 4\n",
    "encoder = Encoder(encoded_space_dim=d, fc2_input_dim=128)\n",
    "decoder = Decoder(encoded_space_dim=d, fc2_input_dim=128)\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d0616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and validation functions:\n",
    "\n",
    "### Training function\n",
    "def train_epoch(encoder, decoder, dataloader, loss_fn, optimizer):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for image_batch, _ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        # Encode data\n",
    "        encoded_data = encoder(image_batch)\n",
    "        # Decode data\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(decoded_data, image_batch)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print batch loss\n",
    "#         print('\\t partial train loss (single batch): %f' % (loss.data))\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "### Testing function\n",
    "def test_epoch(encoder, decoder, dataloader, loss_fn):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        for image_batch, _ in dataloader:\n",
    "            # Encode data\n",
    "            encoded_data = encoder(image_batch)\n",
    "            # Decode data\n",
    "            decoded_data = decoder(encoded_data)\n",
    "            # Append the network output and the original image to the lists\n",
    "            conc_out.append(decoded_data.cpu())\n",
    "            conc_label.append(image_batch.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    return val_loss.data\n",
    "\n",
    "\n",
    "def plot_ae_outputs(encoder,decoder,n=10):\n",
    "    plt.figure(figsize=(16,4.5))\n",
    "    targets = test_dataset.targets.numpy()\n",
    "    t_idx = {i:np.where(targets==i)[0][0] for i in range(n)}\n",
    "    for i in range(n):\n",
    "        \n",
    "        ax = plt.subplot(2,n,i+1)\n",
    "        img = test_dataset[t_idx[i]][0].unsqueeze(0)\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            rec_img  = decoder(encoder(img))\n",
    "        plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)  \n",
    "        if i == n//2:\n",
    "            ax.set_title('Original images')\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)  \n",
    "        if i == n//2:\n",
    "            ax.set_title('Reconstructed images')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3f6e2f",
   "metadata": {},
   "source": [
    "### Question - what is the dimension of the latent space?\n",
    "\n",
    "Ask yourself, how many dimensions we boild the images down to, i.e. how many numbers represent what the images look like?\n",
    "\n",
    "NOTE: You may want to launch the training cell below (which takes a few minutes) and then think about the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "diz_loss = {'train_loss':[],'val_loss':[]}\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(encoder,decoder, train_loader, loss_fn, optim)\n",
    "    val_loss = test_epoch(encoder,decoder,test_loader,loss_fn)\n",
    "    print('\\n EPOCH {}/{} \\t train loss {} \\t val loss {}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
    "    diz_loss['train_loss'].append(train_loss)\n",
    "    diz_loss['val_loss'].append(val_loss)\n",
    "    plot_ae_outputs(encoder,decoder,n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8342aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses over epochs:\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(diz_loss['train_loss'], label='Train')\n",
    "plt.semilogy(diz_loss['val_loss'], label='Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "#plt.title('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d44a3",
   "metadata": {},
   "source": [
    "### Extract latent space representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward pass on test set and subtract latent layer (and labels):\n",
    "latent_test = []\n",
    "labels_test = []\n",
    "for step, (images, labels) in enumerate(test_loader): \n",
    "    with torch.no_grad():\n",
    "        latent = encoder(images)\n",
    "    latent_test.extend(latent.numpy())\n",
    "    labels_test.extend(labels.numpy())  \n",
    "    \n",
    "# labels_str = list(map(str, labels_test))\n",
    "    \n",
    "print(f'latent shape: {np.shape(latent_test)}, labels shape: {np.shape(labels_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74633b70",
   "metadata": {},
   "source": [
    "## Reduce latent space with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddf2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a (possibly parametric!) UMAP embedding to the latent space representation:\n",
    "# embedder = ParametricUMAP(n_epochs=50, n_neighbors=20, min_dist=0.1)\n",
    "embedder = UMAP(n_neighbors=50, min_dist=0.2)\n",
    "embedding = embedder.fit_transform(latent_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78790cee",
   "metadata": {},
   "source": [
    "### Question - can you \"train\" UMAP on a training set, and then apply the same model to a test set?\n",
    "\n",
    "Seeing how t-SNE and UMAP iteratively got to their resulting projections, do you think that at the end of the process there is a model, that can be applied to other data? And if not, can you think about, how one could construct a good approximation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2471548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot embedding without example images:\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cmap = plt.cm.tab10\n",
    "scatter = ax.scatter(embedding[:, 0], embedding[:, 1], c=labels_test, s=10, cmap=cmap)\n",
    "ax.legend(*scatter.legend_elements())\n",
    "ax.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot embedding with example images:\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "cmap = plt.cm.tab10\n",
    "scatter = ax.scatter(embedding[:, 0], embedding[:, 1], c=labels_test, s=10, cmap=cmap)\n",
    "ax.legend(*scatter.legend_elements())\n",
    "image_positions = np.array([[1., 1.]])\n",
    "for index, position in enumerate(embedding):\n",
    "    dist = np.sum((position - image_positions) ** 2, axis=1)\n",
    "    if np.min(dist) > 4:            # If far enough from other images...\n",
    "        image_positions = np.r_[image_positions, [position]]\n",
    "        imagebox = mpl.offsetbox.AnnotationBbox(\n",
    "            mpl.offsetbox.OffsetImage(test_dataset[index][0][0], cmap=\"binary\"),\n",
    "            position, bboxprops={\"edgecolor\": cmap(labels_test[index]), \"lw\": 2})\n",
    "        fig.gca().add_artist(imagebox)\n",
    "ax.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clustering of embedding:\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans.fit(embedding)\n",
    "clusters = pd.DataFrame(kmeans.labels_, columns=['Cluster'])\n",
    "\n",
    "classes = np.array(labels_test)\n",
    "class_names = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "clust0 = clusters.loc[clusters['Cluster'] == 0].index.to_numpy()\n",
    "clust1 = clusters.loc[clusters['Cluster'] == 1].index.to_numpy()\n",
    "clust2 = clusters.loc[clusters['Cluster'] == 2].index.to_numpy()\n",
    "clust3 = clusters.loc[clusters['Cluster'] == 3].index.to_numpy()\n",
    "clust4 = clusters.loc[clusters['Cluster'] == 4].index.to_numpy()\n",
    "clust5 = clusters.loc[clusters['Cluster'] == 5].index.to_numpy()\n",
    "clust6 = clusters.loc[clusters['Cluster'] == 6].index.to_numpy()\n",
    "clust7 = clusters.loc[clusters['Cluster'] == 7].index.to_numpy()\n",
    "clust8 = clusters.loc[clusters['Cluster'] == 8].index.to_numpy()\n",
    "clust9 = clusters.loc[clusters['Cluster'] == 9].index.to_numpy()\n",
    "cluster_list = [clust0, clust1, clust2, clust3, clust4, clust5, clust6, clust7, clust8, clust9]\n",
    "\n",
    "real_labels = np.concatenate([classes[clust0], classes[clust1], classes[clust2], classes[clust3], classes[clust4], classes[clust5],\n",
    "                             classes[clust6], classes[clust7], classes[clust8], classes[clust9]])\n",
    "pred_labels = []\n",
    "\n",
    "for cluster in cluster_list:\n",
    "    unique, pos = np.unique(classes[cluster], return_inverse=True)\n",
    "    maxpos = np.bincount(pos).argmax()\n",
    "    cluster_label = unique[maxpos]\n",
    "    for i in range(len(cluster)):\n",
    "        pred_labels.append(cluster_label)\n",
    "\n",
    "pred_labels = np.array(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37426328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix:\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True,\n",
    "                          savefig = ''):\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=16)\n",
    "    cbar = plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    \n",
    "    \n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45, fontsize=16)\n",
    "        plt.yticks(tick_marks, target_names, fontsize=16)\n",
    "        \n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     fontsize=13,\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     fontsize=16,\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=16)\n",
    "    plt.xlabel(f'Predicted label\\n\\n accuracy={accuracy:0.3f}; misclass={misclass:0.3f}', fontsize=16)\n",
    "    cbar.ax.set_ylabel('Number of items',  labelpad=20, rotation=270, fontsize=16)   \n",
    "    \n",
    "    \n",
    "    if savefig: plt.savefig(savefig, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d26532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix:\n",
    "cm = confusion_matrix(real_labels, pred_labels, labels=class_names)\n",
    "plot_confusion_matrix(cm, target_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad804a5d",
   "metadata": {},
   "source": [
    "### Question - are you satisfied with the confusion matrix?\n",
    "\n",
    "Is the performance \"good enough\"? If not, what do you think is the problem, and how could it be solved in a simple manner? And if this is not enough, discuss what else could be done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9ac1ae",
   "metadata": {},
   "source": [
    "### Final questions - could you learn the number of digits?\n",
    "\n",
    "Imagine that you didn't know the digits (or something else like zebra calls!), and wanted to estimate this. Can you think of how to do that, and would you be sure to get the right result or a reasonable estimate of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6acfb",
   "metadata": {},
   "source": [
    "# Anamoly detection with an AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112baca2",
   "metadata": {},
   "source": [
    "## Noising the images\n",
    "\n",
    "Let's first add some Gaussian noise to the images. Feel free to change the amount of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e032878",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset  = datasets.MNIST('dataset', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split the test data into two sets:\n",
    "anomal = torch.utils.data.Subset(test_dataset, range(500))\n",
    "\n",
    "# Create lists to hold modified images and labels\n",
    "modified_images = []\n",
    "modified_labels = []\n",
    "\n",
    "# Add noise to the images and store them\n",
    "for i in range(len(anomal)):\n",
    "    img, label = anomal[i]\n",
    "    img = img.unsqueeze(0)  # Add batch dimension\n",
    "    noise = torch.randn_like(img) * 0.5  # Generate Gaussian noise\n",
    "    img += noise  # Add noise to the image\n",
    "    img = torch.clamp(img, 0., 1.)  # Ensure pixel values remain in the valid range [0, 1]\n",
    "    modified_images.append(img.squeeze(0))  # Remove batch dimension\n",
    "    modified_labels.append(label)\n",
    "\n",
    "# Create a new dataset with the modified images and labels\n",
    "modified_dataset = [(img, label) for img, label in zip(modified_images, modified_labels)]\n",
    "# Create a DataLoader for the modified dataset\n",
    "modified_loader = DataLoader(modified_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Plot the original and modified images\n",
    "plt.figure(figsize=(16, 4.5))\n",
    "for i in range(10):\n",
    "    ax = plt.subplot(2, 10, i + 1)\n",
    "    img, _ = next(iter(test_loader))\n",
    "    plt.imshow(img[i].cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)  \n",
    "    if i == 10//2:\n",
    "        ax.set_title('Original images')\n",
    "    ax = plt.subplot(2, 10, i + 1 + 10)\n",
    "    img, _ = next(iter(modified_loader))\n",
    "    plt.imshow(img[i].cpu().squeeze().numpy(), cmap='gist_gray')  \n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)  \n",
    "    if i == 10//2:\n",
    "        ax.set_title('Modified images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511ba0b5",
   "metadata": {},
   "source": [
    "# Embedding layer representation of the anomalous image\n",
    "We pass the images through the encoder and see where they land on the latent space of our trained UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16af28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward pass on test set and subtract latent layer (and labels):\n",
    "latent_anomal = []\n",
    "labels_anomal = []\n",
    "for step, (images, labels) in enumerate(modified_loader):\n",
    "    with torch.no_grad():\n",
    "        latent = encoder(images)\n",
    "    latent_anomal.extend(latent.numpy())\n",
    "    labels_anomal.extend(labels.numpy())  \n",
    "    \n",
    "# labels_str = list(map(str, labels_test))\n",
    "    \n",
    "print(f'latent shape: {np.shape(latent_test)}, labels shape: {np.shape(labels_test)}')\n",
    "\n",
    "# Pass the modified images through the embedder:\n",
    "embedding_anomal = embedder.transform(latent_anomal)\n",
    "\n",
    "\n",
    "# Plot the latent space representation of the modified images:\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cmap = plt.cm.tab10\n",
    "scatter = ax.scatter(embedding[:, 0], embedding[:, 1], c=labels_test, s=10, cmap=cmap, label='Original')\n",
    "scatter_anomal = ax.scatter(embedding_anomal[:, 0], embedding_anomal[:, 1], c=labels_anomal, s=10, cmap=cmap, edgecolor='black', label='Anomalies')\n",
    "ax.legend(*scatter.legend_elements(), loc='upper right')\n",
    "\n",
    "ax.set_title('Latent space representation of original and modified images (black edges)')\n",
    "ax.axis(\"off\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a752dca6",
   "metadata": {},
   "source": [
    "## Same thing as above, but with example images\n",
    "The example images are randomly picked and not necessarily representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot embedding with example images:\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "cmap = plt.cm.tab10\n",
    "scatter = ax.scatter(embedding[:, 0], embedding[:, 1], c=labels_test, s=10, cmap=cmap)\n",
    "scatter_anomal = ax.scatter(embedding_anomal[:, 0], embedding_anomal[:, 1], c=labels_anomal, s=10, cmap=cmap, edgecolor='black')\n",
    "ax.legend(*scatter.legend_elements())\n",
    "\n",
    "# Shuffle indices to start at a different point each time\n",
    "indices = list(range(len(embedding_anomal)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "image_positions = np.array([[1., 1.]])\n",
    "for idx in indices:\n",
    "    position = embedding_anomal[idx]\n",
    "    dist = np.sum((position - image_positions) ** 2, axis=1)\n",
    "    if np.min(dist) > 4:            # If far enough from other images...\n",
    "        image_positions = np.r_[image_positions, [position]]\n",
    "        imagebox = mpl.offsetbox.AnnotationBbox(\n",
    "            mpl.offsetbox.OffsetImage(modified_dataset[idx][0][0], cmap=\"binary\"),\n",
    "            position, bboxprops={\"edgecolor\": cmap(labels_anomal[idx]), \"lw\": 2})\n",
    "        fig.gca().add_artist(imagebox)\n",
    "ax.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c494dea3",
   "metadata": {},
   "source": [
    "# Reconstruction using decoder\n",
    "Let's take a look at how the decoder performs on the reconstruction of the noisy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd12d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reconstructed images for the modified images:\n",
    "plt.figure(figsize=(16,4.5))\n",
    "for i in range(10):\n",
    "    ax = plt.subplot(2,10,i+1)\n",
    "    img = modified_dataset[i][0].unsqueeze(0)\n",
    "    label = modified_dataset[i][1]\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        rec_img  = decoder(encoder(img))\n",
    "    plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == 10//2:\n",
    "        ax.set_title('Original images')\n",
    "    # Add label below the image\n",
    "    ax.text(0.5, -0.1, f'True: {label}', ha='center', va='center', transform=ax.transAxes, fontsize=8)\n",
    "\n",
    "    ax = plt.subplot(2, 10, i + 1 + 10)\n",
    "    plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if i == 10//2:\n",
    "        ax.set_title('Reconstructed images')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c9ef6",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "For completeness, let' see how well k-means is at clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32156fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clustering of embedding:\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans.fit(embedding_anomal)\n",
    "clusters = pd.DataFrame(kmeans.labels_, columns=['Cluster'])\n",
    "\n",
    "classes = np.array(labels_test)\n",
    "class_names = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "clust0 = clusters.loc[clusters['Cluster'] == 0].index.to_numpy()\n",
    "clust1 = clusters.loc[clusters['Cluster'] == 1].index.to_numpy()\n",
    "clust2 = clusters.loc[clusters['Cluster'] == 2].index.to_numpy()\n",
    "clust3 = clusters.loc[clusters['Cluster'] == 3].index.to_numpy()\n",
    "clust4 = clusters.loc[clusters['Cluster'] == 4].index.to_numpy()\n",
    "clust5 = clusters.loc[clusters['Cluster'] == 5].index.to_numpy()\n",
    "clust6 = clusters.loc[clusters['Cluster'] == 6].index.to_numpy()\n",
    "clust7 = clusters.loc[clusters['Cluster'] == 7].index.to_numpy()\n",
    "clust8 = clusters.loc[clusters['Cluster'] == 8].index.to_numpy()\n",
    "clust9 = clusters.loc[clusters['Cluster'] == 9].index.to_numpy()\n",
    "cluster_list = [clust0, clust1, clust2, clust3, clust4, clust5, clust6, clust7, clust8, clust9]\n",
    "\n",
    "real_labels = np.concatenate([classes[clust0], classes[clust1], classes[clust2], classes[clust3], classes[clust4], classes[clust5],\n",
    "                             classes[clust6], classes[clust7], classes[clust8], classes[clust9]])\n",
    "pred_labels = []\n",
    "\n",
    "for cluster in cluster_list:\n",
    "    unique, pos = np.unique(classes[cluster], return_inverse=True)\n",
    "    maxpos = np.bincount(pos).argmax()\n",
    "    cluster_label = unique[maxpos]\n",
    "    for i in range(len(cluster)):\n",
    "        pred_labels.append(cluster_label)\n",
    "\n",
    "pred_labels = np.array(pred_labels)\n",
    "\n",
    "# Plot confusion matrix:\n",
    "cm = confusion_matrix(real_labels, pred_labels, labels=class_names)\n",
    "plot_confusion_matrix(cm, target_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055adfb8",
   "metadata": {},
   "source": [
    "# Loss distribution\n",
    "We will now plot the loss of the noisy images against the first two batches of the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dbbedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "image_losses_anom = []\n",
    "image_losses_reg = []\n",
    "with torch.no_grad():\n",
    "    # Iterate through the modified_loader\n",
    "    for images, labels in modified_loader:\n",
    "        for i in range(len(images)):\n",
    "            # Pass the images through the encoder\n",
    "            encoded_data = encoder(images[i].unsqueeze(0))\n",
    "            # Pass the encoded data through the decoder\n",
    "            decoded_data = decoder(encoded_data)\n",
    "            # Calculate the loss between the original and reconstructed images\n",
    "            loss = loss_fn(decoded_data, images[i].unsqueeze(0))\n",
    "            image_losses_anom.append(loss.item())\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "        if batch_idx >= 2:  # Limit to the first two batches\n",
    "            break\n",
    "        for i in range(len(images)):\n",
    "            # Pass the images through the encoder\n",
    "            encoded_data = encoder(images[i].unsqueeze(0))\n",
    "            # Pass the encoded data through the decoder\n",
    "            decoded_data = decoder(encoded_data)\n",
    "            # Calculate the loss between the original and reconstructed images\n",
    "            loss = loss_fn(decoded_data, images[i].unsqueeze(0))\n",
    "            image_losses_reg.append(loss.item())\n",
    "\n",
    "# Plot histogram of reconstruction losses:\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(image_losses_anom, bins=50, color='blue', alpha=0.7)\n",
    "plt.hist(image_losses_reg, bins=50, color='red', alpha=0.7)\n",
    "plt.legend(['Anomalous Images', 'Regular Images'])\n",
    "plt.title('Histogram of Reconstruction Losses')\n",
    "plt.xlabel('Reconstruction Loss')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cac618",
   "metadata": {},
   "source": [
    "# Questions\n",
    "1. With _enough_ noise you should observe that no images get mapped to the 1s' (orange dots) in the embedding space. Why could that be the case?\n",
    "2. Does it surprise you that the reconstructed images have black backgrounds? What does this suggest about the role of background pixels in training the autoencoder?\n",
    "3. How can the reconstruction loss be used as an indicator of anomalies? Under what assumptions is this a reliable measure, and what are some limitations?\n",
    "4. Do the noisy images tend to cluster near their original digit classes in the embedding space, or are they dispersed? How does this influence the success of k-means clustering?\n",
    "5. Try to use anomalies that are **different** from the noisy copies of numbers, e.g. letters. A sample of these can be found in GitHub folder (\"emnist-letters-test.csv\"). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
