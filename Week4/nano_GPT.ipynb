{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lyw35ahWTsk"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/troelspetersen/AppliedML2025_InstructorsFolder/blob/main/Week4/nano_GPT.ipynb)\n",
        "\n",
        "# Building a Character-Level Shakespearean Transformer (nano-GPT)\n",
        "\n",
        "Welcome to this hands-on exercise where we'll build and train a miniature version of a Generative Pre-trained Transformer (GPT) model.\n",
        "\n",
        "This notebook is heavily inspired by and simplifies Andrej Karpathy's fantastic [nanoGPT repository](https://github.com/karpathy/nanoGPT). Our goal is to strip down the complexity to the bare essentials needed to understand the core concepts and train a character-level language model on the works of Shakespeare.\n",
        "\n",
        "**Why is this exciting?**\n",
        "The Transformer architecture, which we'll be implementing, is the foundation of modern Large Language Models (LLMs). While our model will be much smaller, the fundamental building blocks (like self-attention, positional embeddings, and decoder blocks) are conceptually very similar to those in models like GPT-3, which powered the original ChatGPT. By building this \"baby GPT,\" you'll gain a much deeper intuition for how these incredible models work.\n",
        "\n",
        "**What we'll do:**\n",
        "1.  **Data Preparation:** Load Shakespeare's text and create a character-level tokenizer.\n",
        "2.  **Model Definition:** Implement the GPT architecture from scratch, including self-attention and transformer blocks.\n",
        "3.  **Training:** Write a simple training loop to teach our model to predict the next character in a sequence.\n",
        "4.  **Generation:** Use our trained model to generate new, Shakespeare-like text.\n",
        "\n",
        "Make sure your Colab runtime is set to use a GPU for faster training (Runtime -> Change runtime type -> GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdGxvM88P90L",
        "outputId": "98e3954c-ebde-4b20-c365-706737c019f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import time\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import pickle # For saving/loading meta later if needed, though not strictly for tokenizer now\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm # For progress bars\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "BATCH_SIZE = 64\n",
        "BLOCK_SIZE = 256  # Context length\n",
        "MAX_ITERS = 5000\n",
        "EVAL_INTERVAL = 250\n",
        "LEARNING_RATE = 3e-4 # Adjusted from 1e-3, often 3e-4 is a good starting point\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "N_EMBD = 384\n",
        "N_HEAD = 6\n",
        "N_LAYER = 6\n",
        "DROPOUT = 0.2\n",
        "# AdamW optimizer betas\n",
        "BETA1 = 0.9\n",
        "BETA2 = 0.95\n",
        "# Early stopping\n",
        "EARLY_STOPPING_PATIENCE = 5 # Number of evaluation intervals to wait\n",
        "EVAL_ITERS_FOR_LOSS = 100 # Number of batches to average for loss estimation\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(1337)\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGA8_ajuWrJ0"
      },
      "source": [
        "## 1. Data Loading and Tokenization\n",
        "\n",
        "The first step in any machine learning project is to prepare the data. For our character-level language model, this involves:\n",
        "1.  **Downloading the Data:** We'll use the \"Tiny Shakespeare\" dataset, which contains a collection of Shakespeare's works.\n",
        "2.  **Creating a Vocabulary:** We'll identify all unique characters present in the text. This set of unique characters will form our vocabulary.\n",
        "3.  **Building a Tokenizer:** We'll create two simple functions:\n",
        "    *   `encode`: Converts a string of characters into a list of corresponding integer IDs (tokens).\n",
        "    *   `decode`: Converts a list of integer IDs back into a string of characters.\n",
        "4.  **Splitting the Data:** We'll divide the dataset into a training set (for teaching the model) and a validation set (for evaluating its performance on unseen data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-aKr2lTRHv4",
        "outputId": "4276af35-f168-41f9-a013-748c46490d29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input.txt already exists.\n",
            "Length of dataset in characters: 1,115,394\n",
            "All unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Vocabulary size: 65\n",
            "Train data has 1,003,854 tokens\n",
            "Validation data has 111,540 tokens\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Data Loading and Tokenization ---\n",
        "input_file_path = 'input.txt'\n",
        "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "\n",
        "if not os.path.exists(input_file_path):\n",
        "    print(f\"Downloading {input_file_path}...\")\n",
        "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "    print(\"Download complete.\")\n",
        "else:\n",
        "    print(f\"{input_file_path} already exists.\")\n",
        "\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "    data = f.read()\n",
        "print(f\"Length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "# Get all unique characters\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"All unique characters:\", ''.join(chars))\n",
        "print(f\"Vocabulary size: {vocab_size:,}\")\n",
        "\n",
        "# Create mappings from characters to integers and vice-versa\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encoder: take a string, output a list of integers\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "# Decoder: take a list of integers, output a string\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l])\n",
        "\n",
        "# Create train and validation splits\n",
        "n = len(data)\n",
        "train_text = data[:int(n * 0.9)]\n",
        "val_text = data[int(n * 0.9):]\n",
        "\n",
        "# Encode the text and convert to PyTorch tensors\n",
        "train_data = torch.tensor(encode(train_text), dtype=torch.long)\n",
        "val_data = torch.tensor(encode(val_text), dtype=torch.long)\n",
        "\n",
        "print(f\"Train data has {len(train_data):,} tokens\")\n",
        "print(f\"Validation data has {len(val_data):,} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cco935wtRLmE",
        "outputId": "adac6572-f9ff-4475-a949-7a72dbc5941d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: 'Hello, World! This is a test.'\n",
            "Encoded text: [20, 43, 50, 50, 53, 6, 1, 35, 53, 56, 50, 42, 2, 1, 32, 46, 47, 57, 1, 47, 57, 1, 39, 1, 58, 43, 57, 58, 8]\n",
            "Decoded text: 'Hello, World! This is a test.'\n",
            "Tokenizer test passed!\n"
          ]
        }
      ],
      "source": [
        "# --- Tokenizer Experimentation ---\n",
        "sample_text = \"Hello, World! This is a test.\"\n",
        "print(f\"Original text: '{sample_text}'\")\n",
        "\n",
        "encoded_sample = encode(sample_text)\n",
        "print(f\"Encoded text: {encoded_sample}\")\n",
        "\n",
        "decoded_sample = decode(encoded_sample)\n",
        "print(f\"Decoded text: '{decoded_sample}'\")\n",
        "\n",
        "assert sample_text == decoded_sample, \"Encoding/Decoding mismatch!\"\n",
        "print(\"Tokenizer test passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ihert8KRQvf",
        "outputId": "93d4dc3a-0306-41d0-a68e-2ecc91a81cb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input batch shape: torch.Size([64, 256])\n",
            "Target batch shape: torch.Size([64, 256])\n",
            "First sequence in input batch (first 30 tokens): \n",
            "Not Gloucester's death, nor H\n",
            "First sequence in target batch (first 30 tokens): Not Gloucester's death, nor He\n"
          ]
        }
      ],
      "source": [
        "# --- Data Loader ---\n",
        "def get_batch(split):\n",
        "    # Selects the appropriate dataset (train or val)\n",
        "    data_source = train_data if split == 'train' else val_data\n",
        "    # Generates random starting indices for batch_size sequences\n",
        "    ix = torch.randint(len(data_source) - BLOCK_SIZE, (BATCH_SIZE,))\n",
        "    # Extracts input sequences (x)\n",
        "    x = torch.stack([data_source[i:i+BLOCK_SIZE] for i in ix])\n",
        "    # Extracts target sequences (y), which are shifted by one character\n",
        "    y = torch.stack([data_source[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
        "    # Move data to the specified device (CPU or GPU)\n",
        "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "    return x, y\n",
        "\n",
        "# Test get_batch\n",
        "xb, yb = get_batch('train')\n",
        "print(\"Input batch shape:\", xb.shape)\n",
        "print(\"Target batch shape:\", yb.shape)\n",
        "print(\"First sequence in input batch (first 30 tokens):\", decode(xb[0][:30].tolist()))\n",
        "print(\"First sequence in target batch (first 30 tokens):\", decode(yb[0][:30].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW9lylKWW1ao"
      },
      "source": [
        "## 2. Model Definition: The Transformer Architecture\n",
        "\n",
        "Now for the exciting part: building our GPT model! We'll implement the core components of the Transformer architecture, specifically the \"decoder-only\" variant used in GPT models.\n",
        "\n",
        "Key components we will define:\n",
        "*   **`GPTConfig`**: A dataclass to hold all our model hyperparameters (like vocabulary size, embedding dimension, number of layers, etc.).\n",
        "*   **`LayerNorm`**: A normalization layer crucial for stabilizing training in deep networks.\n",
        "*   **`CausalSelfAttention`**: The heart of the Transformer! This module allows each token in a sequence to \"attend\" to previous tokens (but not future ones, hence \"causal\") to understand context. We'll implement the scaled dot-product attention mechanism.\n",
        "*   **`MLP` (Multi-Layer Perceptron)**: A simple feed-forward network applied after the attention mechanism in each Transformer block.\n",
        "*   **`Block`**: A single Transformer block, which typically consists of a self-attention layer followed by an MLP, with residual connections and layer normalization.\n",
        "*   **`GPT`**: The main model class that stacks multiple `Block`s, includes token and positional embeddings, and a final linear layer to predict the next token.\n",
        "\n",
        "We'll also explore the model's output *before* any training to understand its initial (random) state and the expected initial loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5nODNxK-RUdE"
      },
      "outputs": [],
      "source": [
        "# --- 2. Model Definition ---\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = BLOCK_SIZE\n",
        "    vocab_size: int = vocab_size # Will be set by loaded data\n",
        "    n_layer: int = N_LAYER\n",
        "    n_head: int = N_HEAD\n",
        "    n_embd: int = N_EMBD\n",
        "    dropout: float = DROPOUT\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. \"\"\"\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # Key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # Regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        # Causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        # We use register_buffer for parameters that should be part of the model's state\n",
        "        # but are not trained by the optimizer (e.g., a fixed mask).\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                    .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # Manual implementation of attention\n",
        "        # (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf')) # Apply causal mask\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # Re-assemble all head outputs side by side\n",
        "\n",
        "        # Output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU() # Using GELU activation\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))  # Attention with residual connection\n",
        "        x = x + self.mlp(self.ln_2(x))   # MLP with residual connection\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Zy_tgqySRgax"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),      # Token embeddings\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),     # Positional embeddings\n",
        "            drop = nn.Dropout(config.dropout),                        # Dropout layer\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # Transformer blocks\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),        # Final layer norm\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # Language model head\n",
        "\n",
        "        # Weight tying: token embeddings and final linear layer share weights\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "        # Apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        print(f\"Number of trainable parameters: {num_params/1e6:.2f}M\")\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size() # Batch size, sequence length\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # Shape (t)\n",
        "\n",
        "        # Forward the GPT model\n",
        "        tok_emb = self.transformer.wte(idx) # Token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # Position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # If we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x) # (b, t, vocab_size)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        else:\n",
        "            # Inference-time optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # Note: using list [-1] to preserve the time dim -> (b, 1, vocab_size)\n",
        "            loss = None\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        \"\"\"\n",
        "        self.eval() # Set model to evaluation mode\n",
        "        for _ in range(max_new_tokens):\n",
        "            # If the sequence context is growing too long, crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # Forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond) # Loss is None during generation\n",
        "            # Pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # Optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf') # Mask non-top-k logits\n",
        "            # Apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # Append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        self.train() # Set model back to training mode if used elsewhere\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WVoHathRq0E",
        "outputId": "1d615d8f-0d2e-4913-9cd5-049737fe0f0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 10.77M\n",
            "--- Before Training ---\n",
            "Logits shape: torch.Size([64, 256, 65])\n",
            "Initial loss: 4.285391330718994\n",
            "Expected initial loss (approx -ln(1/vocab_size)): 4.1744\n",
            "Logits for the first token prediction in the first sequence (first 10 values): tensor([ 0.0880, -1.0848,  0.1741,  0.4192, -0.1020, -0.2917, -0.1435, -0.2548,\n",
            "        -0.5986,  0.0692], device='cuda:0', grad_fn=<SliceBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# --- Model Initialization and Initial Exploration ---\n",
        "\n",
        "# Create GPTConfig instance\n",
        "model_config = GPTConfig(vocab_size=vocab_size, block_size=BLOCK_SIZE,\n",
        "                         n_layer=N_LAYER, n_head=N_HEAD, n_embd=N_EMBD, dropout=DROPOUT)\n",
        "\n",
        "# Instantiate the model\n",
        "model = GPT(model_config)\n",
        "model.to(DEVICE)\n",
        "\n",
        "# Get a batch of data\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "print(\"--- Before Training ---\")\n",
        "# Pass the batch through the UNTRAINED model\n",
        "logits, loss = model(xb, yb)\n",
        "\n",
        "print(\"Logits shape:\", logits.shape) # Should be (BATCH_SIZE, BLOCK_SIZE, vocab_size)\n",
        "print(\"Initial loss:\", loss.item())\n",
        "\n",
        "# For a randomly initialized model, the loss should be roughly -ln(1/vocab_size)\n",
        "# This is because the model initially assigns roughly equal probability to each token in the vocabulary.\n",
        "# The cross-entropy loss for a uniform distribution over V classes is -sum( (1/V) * log(1/V) ) = -V * (1/V) * log(1/V) = -log(1/V) = log(V)\n",
        "expected_loss = -math.log(1.0 / vocab_size)\n",
        "print(f\"Expected initial loss (approx -ln(1/vocab_size)): {expected_loss:.4f}\")\n",
        "# The actual initial loss will vary due to specific weight initialization but should be in this ballpark.\n",
        "\n",
        "# Let's look at the logits for the first token prediction in the first sequence\n",
        "first_token_logits = logits[0, 0, :]\n",
        "print(\"Logits for the first token prediction in the first sequence (first 10 values):\", first_token_logits[:10])\n",
        "# These are raw, unnormalized scores for each possible next character.\n",
        "# After softmax, these would turn into probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZNucyqirRtaZ"
      },
      "outputs": [],
      "source": [
        "# --- Evaluation Function ---\n",
        "@torch.no_grad() # Decorator to disable gradient calculations during evaluation\n",
        "def estimate_loss(model_to_eval):\n",
        "    out = {}\n",
        "    model_to_eval.eval() # Set the model to evaluation mode\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(EVAL_ITERS_FOR_LOSS) # Array to store losses for averaging\n",
        "        for k in range(EVAL_ITERS_FOR_LOSS):\n",
        "            X, Y = get_batch(split)\n",
        "            _, current_loss = model_to_eval(X, Y)\n",
        "            losses[k] = current_loss.item()\n",
        "        out[split] = losses.mean() # Average loss over EVAL_ITERS_FOR_LOSS batches\n",
        "    model_to_eval.train() # Set the model back to training mode\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Chb-oYW_2W"
      },
      "source": [
        "## 3. Training the Model\n",
        "\n",
        "With our data prepared and model defined, it's time to train! The training process involves:\n",
        "1.  **Loss Estimation:** We'll define a helper function (`estimate_loss`) to calculate the model's performance (cross-entropy loss) on both the training and validation sets without updating its weights. This helps us monitor learning and detect overfitting.\n",
        "2.  **Optimizer:** We'll use the AdamW optimizer, a common choice for training Transformers.\n",
        "3.  **Training Loop:**\n",
        "    *   Repeatedly sample batches of data.\n",
        "    *   Perform a **forward pass**: Feed the input batch to the model to get predictions (logits) and calculate the loss.\n",
        "    *   Perform a **backward pass**: Calculate gradients of the loss with respect to the model's parameters.\n",
        "    *   **Update parameters**: Adjust the model's weights using the optimizer to minimize the loss.\n",
        "4.  **Evaluation & Checkpointing:** Periodically, we'll evaluate the model on the validation set. If the validation loss improves, we'll save a \"checkpoint\" of the model's weights.\n",
        "5.  **Early Stopping:** If the validation loss stops improving for a certain number of evaluations, we'll stop training to prevent overfitting.\n",
        "\n",
        "We'll use `tqdm` to display a progress bar during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g-m4EsXR64X",
        "outputId": "c242293f-8ff1-49bf-ea98-4ce2a00551aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training on cuda...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:   0%|          | 0/5000 [00:32<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step     0: Train loss 4.2999, Val loss 4.3001\n",
            "  -> New best validation loss: 4.3001. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:   5%|▌         | 251/5000 [03:01<13:26:53, 10.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step   250: Train loss 2.3349, Val loss 2.3687\n",
            "  -> New best validation loss: 2.3687. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  10%|█         | 501/5000 [05:29<12:45:03, 10.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step   500: Train loss 1.8924, Val loss 2.0067\n",
            "  -> New best validation loss: 2.0067. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  15%|█▌        | 751/5000 [07:57<12:02:14, 10.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step   750: Train loss 1.6438, Val loss 1.8208\n",
            "  -> New best validation loss: 1.8208. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  20%|██        | 1001/5000 [10:26<11:19:59, 10.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  1000: Train loss 1.4985, Val loss 1.6962\n",
            "  -> New best validation loss: 1.6962. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  25%|██▌       | 1251/5000 [12:54<10:37:33, 10.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  1250: Train loss 1.4084, Val loss 1.6153\n",
            "  -> New best validation loss: 1.6153. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  30%|███       | 1501/5000 [15:22<9:54:52, 10.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  1500: Train loss 1.3477, Val loss 1.5790\n",
            "  -> New best validation loss: 1.5790. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  35%|███▌      | 1751/5000 [17:51<9:12:26, 10.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  1750: Train loss 1.2908, Val loss 1.5349\n",
            "  -> New best validation loss: 1.5349. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  40%|████      | 2001/5000 [20:20<8:29:33, 10.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  2000: Train loss 1.2481, Val loss 1.5075\n",
            "  -> New best validation loss: 1.5075. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  45%|████▌     | 2251/5000 [22:48<7:47:32, 10.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  2250: Train loss 1.2072, Val loss 1.5006\n",
            "  -> New best validation loss: 1.5006. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  50%|█████     | 2501/5000 [25:16<7:04:54, 10.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  2500: Train loss 1.1751, Val loss 1.4952\n",
            "  -> New best validation loss: 1.4952. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  55%|█████▌    | 2751/5000 [27:45<6:22:27, 10.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  2750: Train loss 1.1429, Val loss 1.4878\n",
            "  -> New best validation loss: 1.4878. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  60%|██████    | 3001/5000 [30:14<5:40:34, 10.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  3000: Train loss 1.1121, Val loss 1.4823\n",
            "  -> New best validation loss: 1.4823. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  65%|██████▌   | 3251/5000 [32:42<4:56:41, 10.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  3250: Train loss 1.0786, Val loss 1.4898\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  70%|███████   | 3501/5000 [35:11<4:14:40, 10.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  3500: Train loss 1.0450, Val loss 1.4814\n",
            "  -> New best validation loss: 1.4814. Model saved to best_shakespeare_model.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  75%|███████▌  | 3751/5000 [37:39<3:32:04, 10.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  3750: Train loss 1.0180, Val loss 1.4886\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  80%|████████  | 4001/5000 [40:08<2:49:13, 10.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  4000: Train loss 0.9872, Val loss 1.5047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  85%|████████▌ | 4251/5000 [42:36<2:07:04, 10.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  4250: Train loss 0.9589, Val loss 1.5163\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  90%|█████████ | 4501/5000 [45:05<1:24:37, 10.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  4500: Train loss 0.9287, Val loss 1.5233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  95%|█████████▌| 4750/5000 [47:33<02:30,  1.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step  4750: Train loss 0.8995, Val loss 1.5485\n",
            "Early stopping triggered at iteration 4750 after 5 evaluation intervals without improvement.\n",
            "--- Training Finished ---\n",
            "Total training time: 47.56 minutes\n",
            "Best validation loss achieved: 1.4814\n",
            "Loading best model weights for generation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Training Loop ---\n",
        "print(f\"Starting training on {DEVICE}...\")\n",
        "\n",
        "# Optimizer (AdamW is a common choice for transformers)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "iter_history = []\n",
        "\n",
        "# For timing\n",
        "overall_start_time = time.time()\n",
        "\n",
        "# Wrap the main loop with tqdm for a progress bar\n",
        "for iter_num in tqdm(range(MAX_ITERS), desc=\"Training Progress\"):\n",
        "    # iter_start_time = time.time() # tqdm provides timing per iteration\n",
        "\n",
        "    # Every EVAL_INTERVAL, estimate loss on train and val sets\n",
        "    if iter_num % EVAL_INTERVAL == 0 or iter_num == MAX_ITERS - 1:\n",
        "        losses = estimate_loss(model) # This already sets model.eval() and model.train()\n",
        "        train_loss_history.append(losses['train'].item())\n",
        "        val_loss_history.append(losses['val'].item())\n",
        "        iter_history.append(iter_num)\n",
        "\n",
        "        # tqdm will show iteration/s, so manual timing print might be redundant\n",
        "        # elapsed_iter_time = time.time() - iter_start_time\n",
        "        # print(f\"Step {iter_num}: Train loss {losses['train']:.4f}, Val loss {losses['val']:.4f}, Iter time: {elapsed_iter_time:.2f}s\")\n",
        "\n",
        "        # Log to tqdm's postfix\n",
        "        tqdm.write(f\"Step {iter_num:5d}: Train loss {losses['train']:.4f}, Val loss {losses['val']:.4f}\")\n",
        "\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            epochs_no_improve = 0\n",
        "            # Save the best model checkpoint to the Colab instance's local storage\n",
        "            # This file will be lost if the Colab runtime is disconnected or reset unless moved.\n",
        "            torch.save(model.state_dict(), 'best_shakespeare_model.pth')\n",
        "            tqdm.write(f\"  -> New best validation loss: {best_val_loss:.4f}. Model saved to best_shakespeare_model.pth\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
        "            tqdm.write(f\"Early stopping triggered at iteration {iter_num} after {EARLY_STOPPING_PATIENCE} evaluation intervals without improvement.\")\n",
        "            break # Exit the training loop\n",
        "\n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Forward pass: evaluate loss\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # Backward pass and optimization step\n",
        "    optimizer.zero_grad(set_to_none=True) # Zero out gradients from previous iteration\n",
        "    loss.backward()                       # Compute gradients\n",
        "    optimizer.step()                      # Update model parameters\n",
        "\n",
        "overall_end_time = time.time()\n",
        "print(f\"--- Training Finished ---\")\n",
        "print(f\"Total training time: {(overall_end_time - overall_start_time)/60:.2f} minutes\")\n",
        "print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
        "\n",
        "# Load the best model weights after training is complete (or early stopped)\n",
        "# This ensures the 'model' variable holds the best performing weights for generation.\n",
        "if os.path.exists('best_shakespeare_model.pth'):\n",
        "    print(\"Loading best model weights for generation...\")\n",
        "    model.load_state_dict(torch.load('best_shakespeare_model.pth', map_location=DEVICE))\n",
        "else:\n",
        "    print(\"Warning: No 'best_shakespeare_model.pth' found. Using current model state (likely the last trained state) for generation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWjsFttMXR5e"
      },
      "source": [
        "## 4. Generating Text with the Trained Model\n",
        "\n",
        "After training (or loading our best checkpoint), our model should have learned the patterns and style of Shakespearean English. Now, we can use it as a generative model:\n",
        "1.  **Provide a Prompt:** We'll give the model a starting sequence of characters (a \"prompt\").\n",
        "2.  **Predict Next Character:** The model will predict the probability distribution for the next character.\n",
        "3.  **Sample:** We'll sample a character from this distribution (we can use `temperature` to control randomness and `top_k` to limit choices).\n",
        "4.  **Append and Repeat:** Append the sampled character to our sequence and feed the new, longer sequence back into the model to generate the next character. We repeat this process to generate new text.\n",
        "\n",
        "Let's see if our model can write some \"new\" Shakespeare!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "883M7aH8R-AF",
        "outputId": "3d60b95d-d168-409b-f1e1-f8eb73ab8de1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Generating Shakespeare-like text ---\n",
            "Starting prompt: 'JULIET:\n",
            "O Romeo, Romeo! wherefore art thou Romeo?'\n",
            "\n",
            "--- Generated Text ---\n",
            "JULIET:\n",
            "O Romeo, Romeo! wherefore art thou Romeo?\n",
            "Arise that thy dead cannot live and me?\n",
            "And like a careerful mother and these fight.\n",
            "\n",
            "JULIET:\n",
            "Was this it with thee? why, thou not likege, what'st thus\n",
            "Thy art disguised out of this death of my good?\n",
            "And so I disguised him to make me a king;\n",
            "And I am his care in power to see his wife\n",
            "To avoid the morning standing of his death.\n",
            "And now, faith, in his head charge him, sir,\n",
            "The pretty is not being to me again.\n",
            "\n",
            "EDWARD:\n",
            "Are you are his open sort must not arm.\n",
            "\n",
            "Second Watchman:\n",
            "I love a corse: the senators that waxed\n",
            "From me to suppose our foe: for him, being still,\n",
            "A man, like a man in honour'd of marriage,\n",
            "Which may see the treasure of your trial,\n",
            "Or else not produce home in his king's;\n",
            "For the worthy earth her lives wound his head,\n",
            "His brother to her was contented next,\n",
            "And all a tired courses where we given him sight,\n",
            "And from the false and the prince for this child.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "I cannot never saw my lord.\n",
            "Now take the duke, then, to beat our love,\n",
            "We would wash a wife and play'd \n"
          ]
        }
      ],
      "source": [
        "# --- 4. Generate Text ---\n",
        "print(f\"\\n--- Generating Shakespeare-like text ---\")\n",
        "\n",
        "# You can change the starting prompt\n",
        "start_string = \"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\\n\"\n",
        "# start_string = \"To be, or not to be, that is the question:\\n\"\n",
        "# start_string = \"The king is dead\"\n",
        "\n",
        "print(f\"Starting prompt: '{start_string.strip()}'\")\n",
        "\n",
        "start_ids = encode(start_string)\n",
        "# Unsqueeze to add batch dimension: (seq_len) -> (1, seq_len)\n",
        "x_input = torch.tensor(start_ids, dtype=torch.long, device=DEVICE).unsqueeze(0)\n",
        "\n",
        "# Generate text\n",
        "model.eval() # Set model to evaluation mode for generation\n",
        "with torch.no_grad(): # No need to track gradients during generation\n",
        "    generated_ids = model.generate(x_input,\n",
        "                                   max_new_tokens=1000,\n",
        "                                   temperature=0.8, # Controls randomness: lower is less random, higher is more random\n",
        "                                   top_k=20)       # Considers only the top_k most likely tokens at each step\n",
        "\n",
        "generated_text = decode(generated_ids[0].tolist()) # Decode the first (and only) batch item\n",
        "print(\"\\n--- Generated Text ---\")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "fhFw5opUSBFA",
        "outputId": "e23a64ac-856f-4a31-855c-06e892e91fbf"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if train_loss_history and val_loss_history:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(iter_history, train_loss_history, label='Training Loss')\n",
        "    plt.plot(iter_history, val_loss_history, label='Validation Loss')\n",
        "    plt.xlabel(f'Iteration (x{1})') # Since iter_history stores actual iteration numbers\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss Over Iterations')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No loss history to plot. Did training run?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xrUrvc3XZeg"
      },
      "source": [
        "## Congratulations and Further Exploration!\n",
        "\n",
        "You've successfully built, trained, and generated text with a character-level GPT model! This is a significant achievement and provides a solid foundation for understanding modern Large Language Models.\n",
        "\n",
        "Here are some exercises and avenues for further exploration to deepen your understanding:\n",
        "\n",
        "**1. Deep Dive into the Code:**\n",
        "    *   **Meticulously study the `CausalSelfAttention`, `Block`, and `GPT` classes.** Add print statements inside their `forward` methods to trace the shapes of tensors at each step.\n",
        "    *   **Understand every line:** If any line of code is unclear, research it or ask! Pay close attention to tensor manipulations (e.g., `view`, `transpose`, `@` for matrix multiplication).\n",
        "\n",
        "**2. Tensor Shapes and Outputs:**\n",
        "    *   Before and during training, **print out the shapes** of `q`, `k`, `v`, and `att` in the `CausalSelfAttention` module. How do they relate to `BATCH_SIZE`, `BLOCK_SIZE`, `N_HEAD`, and `N_EMBD`?\n",
        "    *   Examine the `logits` output by the model. What does `logits.shape` tell you? How does `F.cross_entropy` use these logits and the target `yb`?\n",
        "    *   What happens if you change `BLOCK_SIZE`? How does it affect memory and the model's \"context window\"?\n",
        "\n",
        "**3. The Magic of Attention (Advanced):**\n",
        "    *   **Extract Attention Weights:** After training, try to visualize or inspect the attention weights. For a given input sequence, which previous tokens does a particular token \"attend\" to most strongly?\n",
        "        *Hint:* The `att` variable inside `CausalSelfAttention.forward` (after `softmax`) holds the attention probabilities. You'll need to run a forward pass with a specific input and capture this tensor.\n",
        "        ```python\n",
        "        # Example: How to potentially get attention weights for a specific layer/head\n",
        "        # model.eval()\n",
        "        # xb_sample, _ = get_batch('val') # Get a sample batch\n",
        "        # specific_block = model.transformer.h[0].attn # e.g., attention in the first block\n",
        "        \n",
        "        # To get 'att' you might need to modify CausalSelfAttention to return it,\n",
        "        # or use PyTorch hooks (more advanced).\n",
        "        # A simpler way for inspection is to add a temporary print inside forward:\n",
        "        # if some_condition_to_print: print(att[0, 0, :, :].detach().cpu().numpy()) # Example for 1st batch, 1st head\n",
        "        ```\n",
        "    *   **(Challenge):** Try to implement a simplified version of the scaled dot-product attention mechanism (`(Q @ K.T) / sqrt(d_k)`) yourself as a standalone function using basic PyTorch tensor operations. Compare its output for a sample Q, K, V with the `att @ v` part of the `CausalSelfAttention` module (before dropout and projection).\n",
        "\n",
        "**4. Hyperparameter Tuning:**\n",
        "    *   Experiment with `LEARNING_RATE`, `N_LAYER`, `N_HEAD`, `N_EMBD`, `DROPOUT`. How do these changes affect training speed, final validation loss, and the quality of generated text?\n",
        "    *   What happens if you significantly increase `MAX_ITERS` (and adjust `EARLY_STOPPING_PATIENCE` if needed)? Can you get a lower validation loss? (Be mindful of Colab time limits).\n",
        "    *   Try different `temperature` and `top_k` values during generation. How does it change the output?\n",
        "\n",
        "**5. Beyond Characters:**\n",
        "    *   Conceptually, how would you adapt this model to work with words or sub-word units instead of characters? What parts of the code would need to change significantly? (This is a big step, more for conceptual understanding).\n",
        "\n",
        "**6. Come Up With More Exercises!**\n",
        "    *   What other aspects of the model or training process are you curious about? Design an experiment to investigate it! For example:\n",
        "        *   How does weight initialization affect training?\n",
        "        *   What if you remove positional embeddings?\n",
        "        *   Can you train on a different small text dataset?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus: Saving and Loading Your Trained Model\n",
        "\n",
        "During training, our script saves the best performing model (based on validation loss) to a file named `best_shakespeare_model.pth` in the current Colab session's temporary storage.\n",
        "\n",
        "**Important:** Files saved directly in the Colab environment are temporary and will be **deleted** when your Colab runtime is disconnected or reset.\n",
        "\n",
        "Here's how you can permanently save your model and load it back:\n",
        "\n",
        "#### Option 1: Downloading the Model to Your Local Computer\n",
        "\n",
        "Once training is complete and `best_shakespeare_model.pth` has been saved by the script:\n",
        "\n",
        "1.  **Locate the File:** In the Colab interface, click on the \"Files\" icon (looks like a folder) in the left sidebar. You should see `best_shakespeare_model.pth` listed.\n",
        "2.  **Download:** Right-click on `best_shakespeare_model.pth` and select \"Download\". The file will be saved to your computer's default downloads folder.\n",
        "\n",
        "To load this model back into a *new* Colab session (or on your local machine if you have PyTorch set up):\n",
        "1.  Upload the `.pth` file back to the Colab session (using the \"Upload to session storage\" button in the Files tab).\n",
        "2.  Then, in your code, you can load it (make sure your model architecture is defined first):\n",
        "    ```python\n",
        "    # # --- Code to define the GPT model and GPTConfig must be run first ---\n",
        "    # model_config = GPTConfig(vocab_size=vocab_size, ...) # Use the same config as training\n",
        "    # model = GPT(model_config)\n",
        "    # model.to(DEVICE)\n",
        "    #\n",
        "    # # Load the state dictionary\n",
        "    # model_path = 'best_shakespeare_model.pth' # Or whatever you named it when uploading\n",
        "    # if os.path.exists(model_path):\n",
        "    #     model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    #     model.eval() # Set to evaluation mode\n",
        "    #     print(f\"Model loaded from {model_path}\")\n",
        "    # else:\n",
        "    #     print(f\"Model file not found at {model_path}\")\n",
        "    ```\n",
        "\n",
        "#### Option 2 (untested!): Saving the Model to Your Google Drive\n",
        "\n",
        "This is a more robust way to save your model for long-term storage and easy access across Colab sessions.\n",
        "\n",
        "1.  **Mount Your Google Drive:**\n",
        "    Run the following code in a cell. You'll be prompted to authorize Colab to access your Google Drive.\n",
        "    ```python\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    ```\n",
        "\n",
        "2.  **Define a Save Path on Your Drive:**\n",
        "    Choose a folder and filename on your Google Drive. It's good practice to create a specific folder for your models.\n",
        "    ```python\n",
        "    # Example save path (modify as needed)\n",
        "    # This will save to a folder named 'MyModels' in the root of your Google Drive.\n",
        "    # Make sure the 'MyModels' folder exists, or create it.\n",
        "    drive_save_path = '/content/drive/MyDrive/MyModels/best_shakespeare_char_gpt.pth'\n",
        "    \n",
        "    # You might want to create the directory if it doesn't exist:\n",
        "    # import os\n",
        "    # os.makedirs(os.path.dirname(drive_save_path), exist_ok=True)\n",
        "    ```\n",
        "\n",
        "3.  **Modify the Training Loop to Save to Drive (or Save Manually After Training):**\n",
        "    *   **To save during training:** In Cell 9 (Training Loop), change the line:\n",
        "        `torch.save(model.state_dict(), 'best_shakespeare_model.pth')`\n",
        "        to:\n",
        "        `torch.save(model.state_dict(), drive_save_path)`\n",
        "    *   **To save manually after training is complete:**\n",
        "        ```python\n",
        "        # # Assuming 'model' is your trained model and 'drive_save_path' is defined\n",
        "        # torch.save(model.state_dict(), drive_save_path)\n",
        "        # print(f\"Model saved to Google Drive: {drive_save_path}\")\n",
        "        ```\n",
        "\n",
        "4.  **Loading the Model from Google Drive:**\n",
        "    In a new session, after mounting your Drive and defining your model architecture:\n",
        "    ```python\n",
        "    # # --- Mount Drive and define model architecture first ---\n",
        "    # from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    #\n",
        "    # # model_config = GPTConfig(vocab_size=vocab_size, ...)\n",
        "    # # model = GPT(model_config)\n",
        "    # # model.to(DEVICE)\n",
        "    #\n",
        "    # drive_model_path = '/content/drive/MyDrive/MyModels/best_shakespeare_char_gpt.pth' # Use the same path\n",
        "    # if os.path.exists(drive_model_path):\n",
        "    #     model.load_state_dict(torch.load(drive_model_path, map_location=DEVICE))\n",
        "    #     model.eval() # Set to evaluation mode\n",
        "    #     print(f\"Model loaded from Google Drive: {drive_model_path}\")\n",
        "    # else:\n",
        "    #     print(f\"Model file not found at {drive_model_path}\")\n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0v26ISDuSEoo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
